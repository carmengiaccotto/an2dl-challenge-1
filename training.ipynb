{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c1c42806ccd7bc",
   "metadata": {
    "id": "c8c1c42806ccd7bc"
   },
   "source": [
    "# **Pirate Pain Challenge - Hyperparameters Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47a74d0babd730",
   "metadata": {
    "id": "bf47a74d0babd730"
   },
   "source": [
    "## üåê **Google Drive Connection or local mount**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f644bab597852f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f644bab597852f40",
    "outputId": "8b184930-243e-45a3-c73e-8b8961375221"
   },
   "source": [
    "import os\n",
    "\n",
    "isColab = False\n",
    "isKaggle = False\n",
    "\n",
    "# Directory di default\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    if not isColab:\n",
    "        raise ImportError(\"We are not in google colab\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/gdrive\")\n",
    "    current_dir = \"/gdrive/My\\\\ Drive/[2025-2026]\\\\ AN2DL/Challenge\\\\ 1/dataset\"\n",
    "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
    "    %cd $current_dir\n",
    "    isColab = True\n",
    "\n",
    "except ImportError:\n",
    "    # Rilevamento ambiente Kaggle\n",
    "    if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") or os.path.exists(\"/kaggle/working\") or isKaggle:\n",
    "        isKaggle = True\n",
    "        kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-1\"\n",
    "        os.makedirs(kaggle_work_dir, exist_ok=True)\n",
    "        current_dir = kaggle_work_dir\n",
    "        print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
    "    else:\n",
    "        isColab = False\n",
    "        isKaggle = False\n",
    "        print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
    "        local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-1\"\n",
    "        current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
    "        print(f\"Directory corrente impostata a: {current_dir}\")\n",
    "\n",
    "# Cambio directory se non Colab (su Colab √® gi√† fatto con %cd)\n",
    "if not isColab:\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "print(f\"Changed directory to: {current_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "932cdb46ae272e67",
   "metadata": {
    "id": "932cdb46ae272e67"
   },
   "source": [
    "## ‚öôÔ∏è **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b8748f0d28fed8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b8748f0d28fed8d",
    "outputId": "6d618f4e-6b90-44c3-8c4c-55fe5d2d489d"
   },
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# from torchsummary import summary\n",
    "\n",
    "logs_dir = \"tensorboard\"\n",
    "if isColab:\n",
    "    !pkill -f tensorboard\n",
    "else:\n",
    "    # Arresta eventuali processi tensorboard in locale (Windows)\n",
    "    import os\n",
    "\n",
    "    if os.name == 'nt':\n",
    "        try:\n",
    "            import psutil\n",
    "\n",
    "            for proc in psutil.process_iter(['name', 'cmdline']):\n",
    "                name = (proc.info.get('name') or '').lower()\n",
    "                cmd = ' '.join(proc.info.get('cmdline') or []).lower()\n",
    "                if 'tensorboard' in name or 'tensorboard' in cmd:\n",
    "                    try:\n",
    "                        proc.kill()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        except ImportError:\n",
    "            import subprocess\n",
    "\n",
    "            subprocess.run(['taskkill', '/F', '/IM', 'tensorboard.exe'],\n",
    "                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "%load_ext tensorboard\n",
    "if isColab:\n",
    "    !mkdir -p models\n",
    "else:\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8701093d38441af",
   "metadata": {
    "id": "a8701093d38441af"
   },
   "source": [
    "## ‚è≥ **Data Downloading**"
   ]
  },
  {
   "cell_type": "code",
   "id": "1191d5d6f484df56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1191d5d6f484df56",
    "outputId": "d6482cae-e372-4335-8fbd-873ee94d162b"
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# --- 1. Impostazioni ---\n",
    "competition_name = 'an2dl2526c1'\n",
    "dataset_path = 'dataset'\n",
    "if isKaggle:\n",
    "    dataset_path = '/kaggle/input/pirate-pain/dataset'\n",
    "train_file = 'pirate_pain_train.csv'\n",
    "test_file = 'pirate_pain_test.csv'\n",
    "labels_file = 'pirate_pain_train_labels.csv'\n",
    "sample_submission_file = 'sample_submission.csv'\n",
    "\n",
    "# Controlla se il dataset √® gi√† stato scaricato ed estratto\n",
    "if not isKaggle and not isColab and not os.path.exists(os.path.join(dataset_path, train_file)):\n",
    "    # --- 2. Autenticazione e Download ---\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    # Inizializza l'API di Kaggle\n",
    "    # L'autenticazione avviene automaticamente se 'kaggle.json' √® in C:\\\\Users\\\\Bert0ns\\\\.kaggle\\\\\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    print(f\"Download del dataset dalla competizione '{competition_name}'...\")\n",
    "\n",
    "    # Crea la directory di destinazione se non esiste\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # Scarica i file della competizione nella cartella 'dataset'\n",
    "    api.competition_download_files(competition_name, path=dataset_path)\n",
    "\n",
    "    # Estrai i file dall'archivio zip\n",
    "    zip_path = os.path.join(dataset_path, f'{competition_name}.zip')\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Estrazione dei file da '{zip_path}'...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_path)\n",
    "        # Rimuovi il file zip dopo l'estrazione\n",
    "        os.remove(zip_path)\n",
    "        print(\"Estrazione completata e file zip rimosso.\")\n",
    "    else:\n",
    "        print(\"ATTENZIONE: File zip non trovato. Il download potrebbe non essere riuscito.\")\n",
    "else:\n",
    "    print(f\"Il dataset √® gi√† presente nella cartella {dataset_path}. Download saltato.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0189d20bfdbdbfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:59:15.146847Z",
     "start_time": "2025-11-08T09:59:15.142723Z"
    },
    "id": "f0189d20bfdbdbfc"
   },
   "source": [
    "## üîé **Exploration and Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d0c6a84418bf8a0",
   "metadata": {
    "id": "9d0c6a84418bf8a0"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "if not isColab:\n",
    "    dataset_df = pd.read_csv(os.path.join(dataset_path, train_file))\n",
    "    kaggle_test_df = pd.read_csv(os.path.join(dataset_path, test_file))\n",
    "    labels_df = pd.read_csv(os.path.join(dataset_path, labels_file))\n",
    "else:  # Per Simo che usa colab in modo strano\n",
    "    dataset_df = pd.read_csv('pirate_pain_train.csv')\n",
    "    kaggle_test_df = pd.read_csv('pirate_pain_test.csv')\n",
    "    labels_df = pd.read_csv('pirate_pain_train_labels.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab93bdf2d91e1a31",
   "metadata": {
    "id": "ab93bdf2d91e1a31"
   },
   "source": [
    "**Convert data to a memory efficient form**"
   ]
  },
  {
   "cell_type": "code",
   "id": "806aabad03a2142f",
   "metadata": {
    "id": "806aabad03a2142f"
   },
   "source": [
    "na_value = -1\n",
    "text_map = {\n",
    "    'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4,\n",
    "    '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
    "    'unknown': na_value, 'nan': na_value, 'none': na_value, 'n/a': na_value,\n",
    "    'one+peg_leg': 1, 'one+hook_hand': 1, 'one+eye_patch': 1,\n",
    "    'no_pain': 0, 'low_pain': 1, 'high_pain': 2\n",
    "}\n",
    "\n",
    "# Pulisce, normalizza, mappa; fallback a numerico e a cifre estratte\n",
    "columns_to_convert = ['n_legs', 'n_hands', 'n_eyes']\n",
    "for col in columns_to_convert:\n",
    "    dataset_df[col] = dataset_df[col].str.strip().str.lower().map(text_map).astype('int8')\n",
    "    kaggle_test_df[col] = kaggle_test_df[col].str.strip().str.lower().map(text_map).astype('int8')\n",
    "\n",
    "# train_df.head(105760)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0e83165934c5ead",
   "metadata": {
    "id": "c0e83165934c5ead"
   },
   "source": [
    "# Convert data types from float64 to float32 to save memory\n",
    "dataset_df[dataset_df.select_dtypes(include=['float64']).columns] = dataset_df.select_dtypes(\n",
    "    include=['float64']).astype(\n",
    "    'float32')\n",
    "kaggle_test_df[kaggle_test_df.select_dtypes(include=['float64']).columns] = kaggle_test_df.select_dtypes(\n",
    "    include=['float64']).astype(\n",
    "    'float32')\n",
    "\n",
    "# Convert int64 to int32\n",
    "dataset_df[dataset_df.select_dtypes(include=['int64']).columns] = dataset_df.select_dtypes(include=['int64']).astype(\n",
    "    'int32')\n",
    "kaggle_test_df[kaggle_test_df.select_dtypes(include=['int64']).columns] = kaggle_test_df.select_dtypes(\n",
    "    include=['int64']).astype('int32')\n",
    "labels_df[labels_df.select_dtypes(include=['int64']).columns] = labels_df.select_dtypes(include=['int64']).astype(\n",
    "    'int32')\n",
    "\n",
    "# Convert pain surveys to int8\n",
    "dataset_df['pain_survey_1'] = dataset_df['pain_survey_1'].astype('int8')\n",
    "dataset_df['pain_survey_2'] = dataset_df['pain_survey_2'].astype('int8')\n",
    "dataset_df['pain_survey_3'] = dataset_df['pain_survey_3'].astype('int8')\n",
    "dataset_df['pain_survey_4'] = dataset_df['pain_survey_4'].astype('int8')\n",
    "\n",
    "kaggle_test_df['pain_survey_1'] = kaggle_test_df['pain_survey_1'].astype('int8')\n",
    "kaggle_test_df['pain_survey_2'] = kaggle_test_df['pain_survey_2'].astype('int8')\n",
    "kaggle_test_df['pain_survey_3'] = kaggle_test_df['pain_survey_3'].astype('int8')\n",
    "kaggle_test_df['pain_survey_4'] = kaggle_test_df['pain_survey_4'].astype('int8')\n",
    "\n",
    "# Convert labels sample_index to int8\n",
    "labels_df['label'] = labels_df['label'].str.strip().str.lower().map(text_map).astype('int8')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Prepare the Time feature**",
   "id": "dfb49b2077a3e588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Time features: normalized_time + sin/cos ---\n",
    "TIME_FEATURES = ['time_norm', 'time_sin', 'time_cos']\n",
    "# Vengono aggiunte tre nuove colonne continue: time_norm, time_sin, time_cos\n",
    "# Se la sequenza per un sample ha lunghezza variabile, normalizziamo dividendo per il max time per sample.\n",
    "for _df in [dataset_df, kaggle_test_df]:\n",
    "    if 'time' in _df.columns and 'time_norm' not in _df.columns:\n",
    "        max_time = _df.groupby('sample_index')['time'].transform('max').replace(0, 1)\n",
    "        _df['time_norm'] = (_df['time'] / max_time).astype('float32')\n",
    "        _df['time_sin'] = np.sin(2 * np.pi * _df['time_norm']).astype('float32')\n",
    "        _df['time_cos'] = np.cos(2 * np.pi * _df['time_norm']).astype('float32')"
   ],
   "id": "2d75f4faaf0be3ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f43311eda52cde4",
   "metadata": {
    "id": "3f43311eda52cde4"
   },
   "source": [
    "## üîÑ **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4a62dc6b39e0396",
   "metadata": {
    "id": "d4a62dc6b39e0396"
   },
   "source": [
    "VALIDATION_SET_PERCENTAGE = 0.15\n",
    "TEST_SET_PERCENTAGE = 0.1\n",
    "\n",
    "JOINT_COLUMNS = [f'joint_{i:02d}' for i in range(31)]\n",
    "\n",
    "CONTINUOUS_COLS = JOINT_COLUMNS + TIME_FEATURES\n",
    "CATEGORICAL_COLS = ['n_legs', 'n_hands', 'n_eyes', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "\n",
    "COLUMNS_TO_REMOVE = [f'joint_{i:02d}' for i in range(13, 26)] + ['joint_30']\n",
    "#COLUMNS_TO_REMOVE = ['joint_30']\n",
    "\n",
    "COLS_TO_EXCLUDE_FROM_NORMALIZATION = TIME_FEATURES"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "295ac2b85394b9c",
   "metadata": {
    "id": "295ac2b85394b9c"
   },
   "source": [
    "num_classes = len(labels_df['label'].unique())\n",
    "unique_samples = dataset_df['sample_index'].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "add7800109f02509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T10:06:02.823302Z",
     "start_time": "2025-11-08T10:06:02.818073Z"
    },
    "id": "add7800109f02509"
   },
   "source": [
    "#### Remove useless features"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a85d49564acf7ed",
   "metadata": {
    "id": "4a85d49564acf7ed"
   },
   "source": [
    "# @title Remove feature from joint_13 to joint_25 + joint_30\n",
    "df_dataset_reduced = dataset_df.drop(columns=COLUMNS_TO_REMOVE, inplace=False)\n",
    "kaggle_test_df_reduced = kaggle_test_df.drop(columns=COLUMNS_TO_REMOVE, inplace=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a75b6bdd01a5f9d4",
   "metadata": {
    "id": "a75b6bdd01a5f9d4"
   },
   "source": [
    "# Rimuoviamo le colonne eliminate anche dalle nostre liste di colonne\n",
    "CONTINUOUS_COLS_REDUCED = [col for col in CONTINUOUS_COLS if col not in COLUMNS_TO_REMOVE]\n",
    "CATEGORICAL_COLS_REDUCED = [col for col in CATEGORICAL_COLS if col not in COLUMNS_TO_REMOVE]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28115726a9498c16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T10:25:32.314161Z",
     "start_time": "2025-11-08T10:25:32.308813Z"
    },
    "id": "28115726a9498c16"
   },
   "source": [
    "#### Build sequences with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "id": "9de97e0a807adc2a",
   "metadata": {
    "id": "9de97e0a807adc2a"
   },
   "source": [
    "# Define a function to build sequences from the dataset\n",
    "def build_sequences(df, label_df, continuous_cols, categorical_cols, window=200, stride=200):\n",
    "    # Sanity check to ensure the window is divisible by the stride\n",
    "    assert window % stride == 0\n",
    "\n",
    "    # Initialise lists to store sequences and their corresponding labels\n",
    "    dataset_continuous = []\n",
    "    dataset_categorical = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over unique IDs in the DataFrame\n",
    "    for id in df['sample_index'].unique():\n",
    "        # Extract sensor data for the current ID\n",
    "        temp_continuous = df[df['sample_index'] == id][continuous_cols].values\n",
    "        temp_categorical = df[df['sample_index'] == id][categorical_cols].values\n",
    "\n",
    "        # Retrieve the activity label for the current ID\n",
    "        label = label_df[label_df['sample_index'] == id]['label'].values[0]\n",
    "\n",
    "        # Calculate padding length to ensure full windows\n",
    "        padding_len = window - len(temp_continuous) % window\n",
    "\n",
    "        # Create zero padding and concatenate with the data\n",
    "        padding_cont = np.zeros((padding_len, temp_continuous.shape[1]), dtype='float32')\n",
    "        padding_cat = np.zeros((padding_len, temp_categorical.shape[1]),\n",
    "                               dtype='int8')  # Padding con 0 per le categoriche\n",
    "        temp_continuous = np.concatenate((temp_continuous, padding_cont))\n",
    "        temp_categorical = np.concatenate((temp_categorical, padding_cat))\n",
    "\n",
    "        # Build feature windows and associate them with labels\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp_continuous):\n",
    "            dataset_continuous.append(temp_continuous[idx:idx + window])\n",
    "            dataset_categorical.append(temp_categorical[idx:idx + window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "\n",
    "    # Convert lists to numpy arrays for further processing\n",
    "    dataset_continuous = np.array(dataset_continuous)\n",
    "    dataset_categorical = np.array(dataset_categorical)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return dataset_continuous, dataset_categorical, labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed8af00de3bc02e6",
   "metadata": {
    "id": "ed8af00de3bc02e6"
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "        persistent_workers=True if num_workers > 0 else False,  # Mantiene i worker attivi\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "484246425afe5f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T10:30:46.123364Z",
     "start_time": "2025-11-08T10:30:46.113836Z"
    },
    "id": "484246425afe5f5f"
   },
   "source": [
    "## üõ†Ô∏è **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9929d84206ec1de2",
   "metadata": {
    "id": "9929d84206ec1de2"
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def recurrent_summary(model, input_specs):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers. It supports models with multiple inputs.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_specs (list of tuples): A list where each tuple contains the shape\n",
    "                                     and dtype of an input tensor.\n",
    "                                     Example: [((seq_len, features_cont), torch.float32),\n",
    "                                               ((seq_len, features_cat), torch.long)]\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create dummy input tensors with batch_size=1\n",
    "    dummy_inputs = []\n",
    "    for shape, dtype in input_specs:\n",
    "        if dtype in [torch.long, torch.int, torch.int8, torch.int16, torch.int32, torch.int64]:\n",
    "            dummy_inputs.append(torch.zeros(1, *shape, dtype=dtype).to(device))\n",
    "        else:\n",
    "            dummy_inputs.append(torch.randn(1, *shape, dtype=dtype).to(device))\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM, nn.ModuleList)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(*dummy_inputs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e391dae3456274e",
   "metadata": {
    "id": "6e391dae3456274e"
   },
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier with Embedding layer for categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_input_size,\n",
    "            categorical_cardinalities,\n",
    "            embedding_dims,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # 1. Embedding Layers per le feature categoriche\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings, emb_dim)\n",
    "            for num_embeddings, emb_dim in zip(categorical_cardinalities, embedding_dims)\n",
    "        ])\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "\n",
    "        # 2. Calcola la dimensione dell'input per la RNN\n",
    "        rnn_input_size = continuous_input_size + total_embedding_dim\n",
    "\n",
    "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        # 3. Crea il layer ricorrente\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        classifier_input_size = hidden_size * 2 if self.bidirectional else hidden_size\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x_continuous, x_categorical):\n",
    "        \"\"\"\n",
    "        x_continuous shape: (batch_size, seq_length, num_continuous_features)\n",
    "        x_categorical shape: (batch_size, seq_length, num_categorical_features)\n",
    "        \"\"\"\n",
    "        # 1. Applica gli embedding\n",
    "        embedded_features = []\n",
    "        for i, emb_layer in enumerate(self.embedding_layers):\n",
    "            # Prendi la i-esima feature categorica per tutti i timestep\n",
    "            cat_feature = x_categorical[:, :, i]\n",
    "            embedded_features.append(emb_layer(cat_feature))\n",
    "\n",
    "        # 2. Concatena gli embedding\n",
    "        # embedded_features √® una lista di tensori (batch, seq, emb_dim)\n",
    "        # li concateniamo lungo l'ultima dimensione\n",
    "        x_embedded = torch.cat(embedded_features, dim=-1)\n",
    "\n",
    "        # 3. Concatena le feature continue con quelle embedded\n",
    "        x_combined = torch.cat([x_continuous, x_embedded], dim=-1)\n",
    "\n",
    "        # 4. Passa il tensore combinato alla RNN\n",
    "        # rnn_out contiene gli output per ogni timestep\n",
    "        rnn_out, hidden = self.rnn(x_combined)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        # Originale del prof\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "\n",
    "        # 5. Proposto da gemini. Usa l'output dell'ultimo timestep per la classificazione\n",
    "        # rnn_out ha shape (batch_size, seq_length, hidden_size * num_directions)\n",
    "        # Prendiamo l'output dell'ultimo timestep: rnn_out[:, -1, :]\n",
    "        # last_timestep_output = rnn_out[:, -1, :]\n",
    "\n",
    "        # 6. Classifica\n",
    "        # logits = self.classifier(last_timestep_output)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42a2e099723b2e2e",
   "metadata": {
    "id": "42a2e099723b2e2e"
   },
   "source": [
    "## üßÆ **Network and Training Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "id": "6886410170bd226d",
   "metadata": {
    "id": "6886410170bd226d"
   },
   "source": [
    "# Cross-validation\n",
    "K = 5  # Number of splits (5 and 10 are considered good values)\n",
    "N_VAL_SAMPLE_INDEXES = int(VALIDATION_SET_PERCENTAGE * len(unique_samples))\n",
    "N_TEST_SAMPLE_INDEXES = int(TEST_SET_PERCENTAGE * len(unique_samples))\n",
    "\n",
    "# Training\n",
    "EPOCHS = 500  # Maximum epochs (increase to improve performance)\n",
    "PATIENCE = 50  # Early stopping patience (increase to improve performance)\n",
    "VERBOSE = 20  # Print frequency\n",
    "\n",
    "# Optimisation\n",
    "LEARNING_RATE = 1e-3  # Learning rate\n",
    "BATCH_SIZE = 512  # Batch size\n",
    "WINDOW_SIZE = 25  # Input window size\n",
    "STRIDE = 5  # Input stride\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2  # Hidden layers\n",
    "HIDDEN_SIZE = 128  # Neurons per layer\n",
    "RNN_TYPE = 'LSTM'  # Type of RNN architecture\n",
    "BIDIRECTIONAL = True  # Bidirectional RNN\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.35  # Dropout probability\n",
    "L1_LAMBDA = 0  # L1 penalty\n",
    "L2_LAMBDA = 1e-4  # L2 penalty\n",
    "\n",
    "# Label smoothing\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# Gradient Clipping\n",
    "MAX_GRADIENT_NORM = 1.0\n",
    "\n",
    "# Embedding dims for categorical dimensions\n",
    "MIN_N_EMBEDDING_DIMS = 4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb5d375c8634b150",
   "metadata": {
    "id": "fb5d375c8634b150"
   },
   "source": [
    "# Definiamo le cardinalit√† (numero di valori unici) per ogni feature categorica.\n",
    "# La cardinalit√† deve essere il valore massimo della categoria + 1.\n",
    "# Questo assicura che tutti gli indici siano validi per il layer di embedding.\n",
    "categorical_cardinalities = [\n",
    "    int(df_dataset_reduced['n_legs'].max() + 1),\n",
    "    int(df_dataset_reduced['n_hands'].max() + 1),\n",
    "    int(df_dataset_reduced['n_eyes'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_1'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_2'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_3'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_4'].max() + 1)\n",
    "]\n",
    "\n",
    "# Definiamo la dimensione dell'embedding per ogni feature.\n",
    "embedding_dims = [max(MIN_N_EMBEDDING_DIMS, (c + 1) // 2) for c in categorical_cardinalities]  #= [2,2,2,2,2,2,2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fd6952bc77c898d",
   "metadata": {
    "id": "5fd6952bc77c898d"
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "EXPERIMENT_NAME = \"lstm_baseline_timeFeatures\"  #spostato qui che mi ero rotto di scorrere #Legittimo bisogno\n",
    "\n",
    "# Set up TensorBoard logging and save model architecture\n",
    "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + EXPERIMENT_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15a61df8bb3c299b",
   "metadata": {
    "id": "15a61df8bb3c299b"
   },
   "source": [
    "# Define parameters to search\n",
    "param_grid = {\n",
    "\n",
    "}\n",
    "\n",
    "# Fixed hyperparameters (not being tuned)\n",
    "fixed_params = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'stride': STRIDE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'l1_lambda': L1_LAMBDA,\n",
    "    'l2_lambda': L2_LAMBDA,\n",
    "    'rnn_type': RNN_TYPE,\n",
    "    'bidirectional': BIDIRECTIONAL,\n",
    "    'embedding_dims': embedding_dims,\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'continuous_cols': CONTINUOUS_COLS_REDUCED,\n",
    "    'categorical_cols': CATEGORICAL_COLS_REDUCED,\n",
    "    'labels_df': labels_df,\n",
    "    'hidden_layers': HIDDEN_LAYERS,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "}\n",
    "\n",
    "# Cross-validation settings\n",
    "cv_params = {\n",
    "    'epochs': EPOCHS,\n",
    "    'device': device,\n",
    "    'k': K,\n",
    "    'n_val_sample_indexes': N_VAL_SAMPLE_INDEXES,\n",
    "    'n_test_sample_indexes': N_TEST_SAMPLE_INDEXES,\n",
    "    'patience': PATIENCE,\n",
    "    'verbose': VERBOSE,\n",
    "    'seed': SEED,\n",
    "    'evaluation_metric': \"val_f1\",\n",
    "    'mode': 'max',\n",
    "    'restore_best_weights': True,\n",
    "    'writer': writer,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "587cbc3f9aa12b85",
   "metadata": {
    "id": "587cbc3f9aa12b85"
   },
   "source": [
    "## üß† **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286446591f8f3aaa",
   "metadata": {
    "id": "286446591f8f3aaa"
   },
   "source": [
    "### **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2ff757937e5c33f",
   "metadata": {
    "id": "d2ff757937e5c33f"
   },
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs_cont, inputs_cat, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs_cont, inputs_cat, targets = inputs_cont.to(device), inputs_cat.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(\n",
    "                device.type == 'cuda')):  # consider to add dtype=torch.float16 to improve speed\n",
    "            logits = model(inputs_cont, inputs_cat)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # --- Gradient Clipping ---\n",
    "        # Unscale gradients before clipping to avoid clipping scaled gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Clip the gradients to a maximum norm (e.g., 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_GRADIENT_NORM)\n",
    "        # --- End of Clipping ---\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs_cont.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a34989395bcf9b2a",
   "metadata": {
    "id": "a34989395bcf9b2a"
   },
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy, all_predictions_np, all_targets_np) -\n",
    "               Validation loss, accuracy, and numpy arrays of predictions and true labels for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs_cont, inputs_cat, targets in val_loader:\n",
    "            # Sposta i dati sul dispositivo corretto\n",
    "            inputs_cont, inputs_cat, targets = inputs_cont.to(device), inputs_cat.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs_cont, inputs_cat)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs_cont.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "\n",
    "    all_predictions_np = np.concatenate(all_predictions)\n",
    "    all_targets_np = np.concatenate(all_targets)\n",
    "\n",
    "    epoch_accuracy = f1_score(\n",
    "        all_targets_np,\n",
    "        all_predictions_np,\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy, all_predictions_np, all_targets_np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "851dd9f5c1a6c70c",
   "metadata": {
    "id": "851dd9f5c1a6c70c"
   },
   "source": [
    "def log_metrics_to_tensorboard(writer: SummaryWriter, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    \"\"\"\n",
    "    Log training metrics and model parameters to TensorBoard for visualization.\n",
    "\n",
    "    Args:\n",
    "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
    "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
    "        train_loss (float): Training loss for this epoch\n",
    "        train_f1 (float): Training f1 score for this epoch\n",
    "        val_loss (float): Validation loss for this epoch\n",
    "        val_f1 (float): Validation f1 score for this epoch\n",
    "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
    "\n",
    "    Note:\n",
    "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
    "        parameters and gradients, which helps monitor training progress and detect\n",
    "        issues like vanishing/exploding gradients.\n",
    "    \"\"\"\n",
    "    # Log scalar metrics\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Check if the tensor is not empty before adding a histogram\n",
    "            if param.numel() > 0:\n",
    "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
    "            if param.grad is not None:\n",
    "                # Check if the gradient tensor is not empty before adding a histogram\n",
    "                if param.grad.numel() > 0:\n",
    "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
    "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "630fe43b1e6922dc",
   "metadata": {
    "id": "630fe43b1e6922dc"
   },
   "source": [
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history, best_val_preds_np, best_val_targets_np) -\n",
    "               Trained model, metrics history, best validation predictions, and best validation true labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    best_val_preds_np = np.array([])\n",
    "    best_val_targets_np = np.array([])\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1, current_val_preds, current_val_targets = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Write metrics to TensorBoard for visualization\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                      f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                      f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\" + experiment_name + '_model.pt')\n",
    "                best_val_preds_np = current_val_preds\n",
    "                best_val_targets_np = current_val_targets\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\" + experiment_name + '_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\" + experiment_name + '_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history, best_val_preds_np, best_val_targets_np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "814f404d42804aef"
   },
   "cell_type": "code",
   "source": [
    "def get_max_score(scores):\n",
    "    \"\"\"\n",
    "    Extract the maximum score from a dictionary of scores.\n",
    "\n",
    "    Args:\n",
    "        scores: Dict with keys like 'split_0', 'split_1', ..., 'mean', 'std'\n",
    "\n",
    "    Returns:\n",
    "        max_score: Maximum score across splits\n",
    "    \"\"\"\n",
    "    split_scores = [v for k, v in scores.items() if k.startswith('split_')]\n",
    "    return max(split_scores) if split_scores else None"
   ],
   "id": "814f404d42804aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c563e24c8fd3f8a",
   "metadata": {
    "id": "2c563e24c8fd3f8a"
   },
   "source": [
    "### **Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5764e050e790fa5b",
   "metadata": {
    "id": "5764e050e790fa5b"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def k_shuffle_split_cross_validation_round_rnn(\n",
    "        df,\n",
    "        labels_df,\n",
    "        continuous_cols,\n",
    "        categorical_cols,\n",
    "        device,\n",
    "        n_val_sample_indexes,\n",
    "        n_test_sample_indexes,\n",
    "        hidden_layers,\n",
    "        hidden_size,\n",
    "        cols_to_exclude_from_normalization=None,\n",
    "        epochs=500,\n",
    "        k=5,\n",
    "        batch_size=512,\n",
    "        learning_rate=1e-3,\n",
    "        dropout_rate=0.2,\n",
    "        window_size=200,\n",
    "        stride=50,\n",
    "        rnn_type='GRU',\n",
    "        bidirectional=False,\n",
    "        embedding_dims=embedding_dims,\n",
    "        l1_lambda=0,\n",
    "        l2_lambda=0,\n",
    "        patience=50,\n",
    "        evaluation_metric=\"val_f1\",\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        writer=None,\n",
    "        verbose=10,\n",
    "        seed=42,\n",
    "        experiment_name=\"rnn_k_shuffle_split\",\n",
    "        label_smoothing=0.1,\n",
    "        plot_confusion_matrix=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    K-shuffle-split cross validation per il dataset Pirate Pain.\n",
    "    Split basato su sample_index (nessuna leakage tra split).\n",
    "    Costruisce sequenze separate in feature continue e categoriche gi√† previste dal modello.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame con le feature (incluso sample_index)\n",
    "        labels_df: DataFrame con colonne ['sample_index','label']\n",
    "        continuous_cols: lista colonne continue\n",
    "        categorical_cols: lista colonne categoriche (gi√† integer-encoded, valori negativi verranno rimappati a 0)\n",
    "        epochs, device, k, ... : come da pipeline esistente\n",
    "        embedding_dims: lista dimensioni embedding (se None calcolate automaticamente)\n",
    "    Returns:\n",
    "        fold_losses: dict split_i -> lista val_loss per epoca\n",
    "        fold_metrics: dict split_i -> lista val_f1 per epoca\n",
    "        best_scores: dict con best f1 per split + mean/std\n",
    "    \"\"\"\n",
    "\n",
    "    # Sanity\n",
    "    if cols_to_exclude_from_normalization is None:\n",
    "        cols_to_exclude_from_normalization = []\n",
    "    assert all(c in df.columns for c in continuous_cols), \"Colonne continue mancanti\"\n",
    "    assert all(c in df.columns for c in categorical_cols), \"Colonne categoriche mancanti\"\n",
    "    assert 'sample_index' in df.columns, \"Manca sample_index\"\n",
    "\n",
    "    # Initialise containers for results across all splits\n",
    "    fold_losses = {}\n",
    "    fold_metrics = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    # Inizializza modello\n",
    "    model = RecurrentClassifier(\n",
    "        continuous_input_size=len(continuous_cols),\n",
    "        categorical_cardinalities=categorical_cardinalities,\n",
    "        embedding_dims=embedding_dims,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=hidden_layers,\n",
    "        num_classes=num_classes,\n",
    "        rnn_type=rnn_type,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Store initial weights to reset model for each split\n",
    "    initial_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Create directory for model checkpoints\n",
    "    os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
    "\n",
    "    for split_idx in range(k):\n",
    "        if verbose > 0:\n",
    "            print(f\"Split {split_idx + 1}/{k}\")\n",
    "\n",
    "        # Reset model to initial weights for fair comparison across splits\n",
    "        model.load_state_dict(initial_state)\n",
    "\n",
    "        # Stratified split su sample_index per mantenere la distribuzione delle label\n",
    "        labels_map = labels_df.set_index('sample_index')['label']\n",
    "        y_all = np.array([labels_map[sid] for sid in unique_samples], dtype=np.int64)\n",
    "\n",
    "        # Primo split: train vs (val+test)\n",
    "        train_ids, temp_ids, y_train, y_temp = train_test_split(\n",
    "            unique_samples,\n",
    "            y_all,\n",
    "            test_size=n_val_sample_indexes + n_test_sample_indexes,\n",
    "            stratify=y_all,\n",
    "            random_state=seed + split_idx,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Secondo split: val vs test\n",
    "        val_ids, test_ids, y_val, y_test = train_test_split(\n",
    "            temp_ids,\n",
    "            y_temp,\n",
    "            test_size=n_test_sample_indexes,\n",
    "            stratify=y_temp,\n",
    "            random_state=seed + split_idx,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        n_train_samples = len(train_ids)\n",
    "        assert n_train_samples > 0, \"Train set vuoto, riduci val/test\"\n",
    "\n",
    "        df_train = df[df['sample_index'].isin(train_ids)].copy()\n",
    "        df_val = df[df['sample_index'].isin(val_ids)].copy()\n",
    "\n",
    "        # Crea il dataset di training, che calcoler√† e salver√† il suo scaler\n",
    "        scaler = StandardScaler()  #MinMaxScaler()  Normalizzo i dati usando media e varianza per vedere se cambia qualcosa\n",
    "        features_to_normalize = list(set(continuous_cols) - set(cols_to_exclude_from_normalization))\n",
    "        df_train[features_to_normalize] = scaler.fit_transform(df_train[features_to_normalize])\n",
    "        df_val[features_to_normalize] = scaler.transform(df_val[features_to_normalize])\n",
    "\n",
    "        # Salva lo scaler per l'inferenza futura\n",
    "        scaler_path = f\"models/{experiment_name}/split_{split_idx}_scaler.pkl\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "        # Creazione delle sequenze con build_sequences\n",
    "        X_train_cont, X_train_cat, y_train = build_sequences(df_train, labels_df, continuous_cols, categorical_cols,\n",
    "                                                             window_size, stride)\n",
    "\n",
    "        X_val_cont, X_val_cat, y_val = build_sequences(df_val, labels_df, continuous_cols, categorical_cols,\n",
    "                                                       window_size, stride)\n",
    "\n",
    "        # Creazione di TensorDataset e DataLoader\n",
    "        train_ds = TensorDataset(torch.from_numpy(X_train_cont).float(), torch.from_numpy(X_train_cat).long(),\n",
    "                                 torch.from_numpy(y_train).long())\n",
    "        val_ds = TensorDataset(torch.from_numpy(X_val_cont).float(), torch.from_numpy(X_val_cat).long(),\n",
    "                               torch.from_numpy(y_val).long())\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Train sequences: {len(train_ds)} | Val sequences: {len(val_ds)}\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=False,\n",
    "                                   drop_last=False)  # drop_last=True pu√≤ aiutare\n",
    "        val_loader = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Calculate class weights for imbalanced dataset\n",
    "        # Calcolo pesi per class imbalance\n",
    "        class_counts = np.bincount(y_train)\n",
    "        total_samples = len(y_train)\n",
    "        class_weights = total_samples / (len(np.unique(y_train)) * class_counts)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n",
    "\n",
    "        # Define optimizer with L2 regularization\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "        # Enable mixed precision training for GPU acceleration\n",
    "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "        _, training_history, best_val_preds_split, best_val_targets_split = fit(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=epochs,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scaler=split_scaler,\n",
    "            device=device,\n",
    "            writer=writer,\n",
    "            patience=patience,\n",
    "            verbose=verbose,\n",
    "            l1_lambda=l1_lambda,\n",
    "            l2_lambda=l2_lambda,\n",
    "            evaluation_metric=evaluation_metric,\n",
    "            mode=mode,\n",
    "            restore_best_weights=restore_best_weights,\n",
    "            experiment_name=f\"{experiment_name}/split_{split_idx}\"\n",
    "        )\n",
    "\n",
    "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
    "        fold_metrics[f\"split_{split_idx}\"] = training_history[evaluation_metric]\n",
    "        best_scores[f\"split_{split_idx}\"] = max(training_history[evaluation_metric])\n",
    "\n",
    "        # Plot confusion matrix for this split's best validation performance\n",
    "        if plot_confusion_matrix and len(best_val_preds_split) > 0:\n",
    "            cm_val = confusion_matrix(best_val_targets_split, best_val_preds_split)\n",
    "            labels_val = np.array([f\"{num}\" for num in cm_val.flatten()]).reshape(cm_val.shape)\n",
    "\n",
    "            plt.figure(figsize=(8, 7))\n",
    "            sns.heatmap(cm_val, annot=labels_val, fmt='', cmap='Blues')\n",
    "            plt.xlabel('Predicted labels')\n",
    "            plt.ylabel('True labels')\n",
    "            plt.title(f'Confusion Matrix - Validation Set (Split {split_idx + 1})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    best_scores[\"mean\"] = float(np.mean(list(best_scores.values())))\n",
    "    best_scores[\"std\"] = float(np.std(list(best_scores.values())))\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"\\n--- K-fold Validation Results ---\")\n",
    "        print(\n",
    "            f\"Best val_F1 score : {get_max_score(best_scores):.4f}, mean: {best_scores['mean']:.4f} \\u00b1 {best_scores['std']:.4f}\")\n",
    "\n",
    "    return fold_losses, fold_metrics, best_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8fe35b66342753",
   "metadata": {
    "id": "c8fe35b66342753"
   },
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True, plot_confusion_matrix=False):\n",
    "    \"\"\"\n",
    "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
    "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
    "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
    "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
    "        verbose: Print progress for each configuration\n",
    "\n",
    "    Returns:\n",
    "        results: Dict with scores for each configuration\n",
    "        best_config: Dict with best hyperparameter combination\n",
    "        best_score: Best mean F1 score achieved\n",
    "    \"\"\"\n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    combinations = list(product(*param_values))\n",
    "\n",
    "    results = {}\n",
    "    best_score = -np.inf\n",
    "    best_config = None\n",
    "\n",
    "    total = len(combinations)\n",
    "\n",
    "    for idx, combo in enumerate(combinations, 1):\n",
    "        # Create current configuration dict\n",
    "        current_config = dict(zip(param_names, combo))\n",
    "        config_str = EXPERIMENT_NAME + \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()]).replace('.', 'p')\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
    "            for param, value in current_config.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "        # Merge current config with fixed parameters\n",
    "        run_params = {**fixed_params, **current_config}\n",
    "\n",
    "        # Execute cross-validation\n",
    "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
    "            df=df,\n",
    "            experiment_name=config_str,\n",
    "            plot_confusion_matrix=plot_confusion_matrix,\n",
    "            cols_to_exclude_from_normalization=COLS_TO_EXCLUDE_FROM_NORMALIZATION,\n",
    "            **run_params,\n",
    "            **cv_params\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        results[config_str] = {**fold_scores}\n",
    "\n",
    "        # Track best configuration\n",
    "        current_max_score = get_max_score(fold_scores)\n",
    "        if current_max_score > best_score:\n",
    "            best_score = current_max_score\n",
    "            best_config = current_config.copy()\n",
    "            if verbose:\n",
    "                print(\" ---NEW BEST SCORE!---\\n\")\n",
    "\n",
    "    return results, best_config, best_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "479512efe289b8b0"
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
    "    \"\"\"\n",
    "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
    "\n",
    "    Args:\n",
    "        results: Dict of results from grid_search_cv_rnn\n",
    "        k_splits: Number of CV splits used\n",
    "        top_n: Number of top configurations to display\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    # Sort by max score across splits\n",
    "    config_scores = {\n",
    "        name: max(v for k, v in data.items() if k.startswith('split_'))\n",
    "        for name, data in results.items()\n",
    "    }\n",
    "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top N\n",
    "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
    "\n",
    "    # Prepare boxplot data\n",
    "    boxplot_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
    "    replacements = {\n",
    "        'batch_size_': 'BS=',\n",
    "        'learning_rate_': '\\nLR=',\n",
    "        'hidden_layers_': '\\nHL=',\n",
    "        'hidden_size_': '\\nHS=',\n",
    "        'dropout_rate_': '\\nDR=',\n",
    "        'window_size_': '\\nWS=',\n",
    "        'stride_': '\\nSTR=',\n",
    "        'rnn_type_': '\\nRNN=',\n",
    "        'bidirectional_': '\\nBIDIR=',\n",
    "        'l1_lambda_': '\\nL1=',\n",
    "        'l2_lambda_': '\\nL2='\n",
    "    }\n",
    "\n",
    "    # Replacements for separators\n",
    "    separator_replacements = {\n",
    "        '_learning_rate_': '\\nLR=',\n",
    "        '_hidden_layers_': '\\nHL=',\n",
    "        '_hidden_size_': '\\nHS=',\n",
    "        '_dropout_rate_': '\\nDR=',\n",
    "        '_window_size_': '\\nWS=',\n",
    "        '_stride_': '\\nSTR=',\n",
    "        '_rnn_type_': '\\nRNN=',\n",
    "        '_bidirectional_': '\\nBIDIR=',\n",
    "        '_l1_lambda_': '\\nL1=',\n",
    "        '_l2_lambda_': '\\nL2=',\n",
    "        '_': ''\n",
    "    }\n",
    "\n",
    "    for config_name, best_score in top_configs:\n",
    "        # Extract best score from each split (auto-detect number of splits)\n",
    "        split_scores = []\n",
    "        for i in range(k_splits):\n",
    "            if f'split_{i}' in results[config_name]:\n",
    "                split_scores.append(results[config_name][f'split_{i}'])\n",
    "        boxplot_data.append(split_scores)\n",
    "\n",
    "        # Verify we have the expected number of splits\n",
    "        if len(split_scores) != k_splits:\n",
    "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
    "\n",
    "        # Create readable label using the replacements dictionary\n",
    "        readable_label = config_name\n",
    "        for old, new in replacements.items():\n",
    "            readable_label = readable_label.replace(old, new)\n",
    "\n",
    "        # Apply separator replacements\n",
    "        for old, new in separator_replacements.items():\n",
    "            readable_label = readable_label.replace(old, new)\n",
    "\n",
    "        labels.append(f\"{readable_label}\\n(max={best_score:.3f})\")\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
    "                    showmeans=True, meanline=True)\n",
    "\n",
    "    # Styling\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    # Highlight best configuration\n",
    "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
    "\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_xlabel('Configuration')\n",
    "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "479512efe289b8b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "97802264915f35ad"
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Function to save and print results in a readable format\n",
    "def save_grid_search_results(results, best_config, best_score, experiment_name=EXPERIMENT_NAME):\n",
    "    payload = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"best_score\": float(best_score),\n",
    "        \"best_config\": best_config,\n",
    "        \"results\": results\n",
    "    }\n",
    "    json_path = f\"{experiment_name}.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON file saved: {json_path}\")\n",
    "    print(\"\\n--- Grid Search Summary ---\")\n",
    "    print(f\"Best F1 (max split): {best_score:.4f}\")\n",
    "    print(\"Best configuration:\")\n",
    "    for k, v in best_config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"\\nAll configurations (mean ¬± std):\")\n",
    "    for cfg_name, data in results.items():\n",
    "        mean_score = data.get(\"mean\")\n",
    "        std_score = data.get(\"std\")\n",
    "        if mean_score is not None and std_score is not None:\n",
    "            print(f\"  {cfg_name}: {mean_score:.4f} ¬± {std_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {cfg_name}: values not available\")"
   ],
   "id": "97802264915f35ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "id": "f7372398f2c512f9"
   },
   "cell_type": "markdown",
   "source": [
    "## **Grid Search**"
   ],
   "id": "f7372398f2c512f9"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1ea449378b6ddecb",
    "outputId": "b0c71a18-3591-4b48-c2a2-8b9ef981e20d"
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Execute search\n",
    "results, best_config, best_score = grid_search_cv_rnn(\n",
    "    df=df_dataset_reduced,\n",
    "    param_grid=param_grid,\n",
    "    fixed_params=fixed_params,\n",
    "    cv_params=cv_params,\n",
    "    plot_confusion_matrix=False  #Cambia qui se vuoi vedere le confusion matrix durante la grid search\n",
    ")"
   ],
   "id": "1ea449378b6ddecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d79142114199aff8",
    "outputId": "69e3ee75-7edc-4e2e-d54f-111bdd54ec7c"
   },
   "cell_type": "code",
   "source": [
    "# Save and print\n",
    "save_grid_search_results(results, best_config, best_score)"
   ],
   "id": "d79142114199aff8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14959bf9db418bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "14959bf9db418bee",
    "outputId": "c1e8dd1c-8cd8-4e8c-da4d-949db4396d68"
   },
   "source": [
    "# Visualise results\n",
    "plot_top_configurations_rnn(results, k_splits=K, top_n=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e9e2d20f6aea468d"
   },
   "cell_type": "code",
   "source": [
    "# --- 1. Combine fixed and best hyperparameters ---\n",
    "# 'fixed_params' and 'best_config' are loaded from the grid search cell\n",
    "final_best_params = {**fixed_params, **best_config}\n",
    "\n",
    "# Generate config string (from grid params only) to find saved model files\n",
    "# If best_config is empty (no params tuned), use EXPERIMENT_NAME directly as the key\n",
    "if not best_config:\n",
    "    best_config_str = EXPERIMENT_NAME\n",
    "else:\n",
    "    best_config_str = EXPERIMENT_NAME + \"_\".join([f\"{k}_{v}\" for k, v in best_config.items()]).replace('.', 'p')"
   ],
   "id": "e9e2d20f6aea468d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "839e2d5dc96e5b26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "839e2d5dc96e5b26",
    "outputId": "cf992972-0fd1-4b23-80c0-022679f7c40f"
   },
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "features_to_normalize = list(set(CONTINUOUS_COLS_REDUCED) - set(COLS_TO_EXCLUDE_FROM_NORMALIZATION))\n",
    "\n",
    "# Initialise lists for metrics\n",
    "test_accuracies = []\n",
    "test_precisions = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "all_test_targets = []  # For aggregated confusion matrix\n",
    "all_test_preds = []  # For aggregated confusion matrix\n",
    "\n",
    "# --- 2. Begin evaluation loop across the K splits ---\n",
    "for split in range(K):\n",
    "    print(f\"Evaluating Split {split + 1}/{K} using best config: {best_config_str}\")\n",
    "\n",
    "    unique_samples = df_dataset_reduced['sample_index'].unique()\n",
    "\n",
    "    # --- 3. Regenerate the exact data split for this fold ---\n",
    "    # This logic must be identical to k_shuffle_split_cross_validation_round_rnn\n",
    "    # Stratified split su sample_index per mantenere la distribuzione delle label\n",
    "    labels_map = labels_df.set_index('sample_index')['label']\n",
    "    y_all = np.array([labels_map[sid] for sid in unique_samples], dtype=np.int64)\n",
    "\n",
    "    # Primo split: train vs (val+test)\n",
    "    train_ids, temp_ids, y_train, y_temp = train_test_split(\n",
    "        unique_samples,\n",
    "        y_all,\n",
    "        test_size=N_VAL_SAMPLE_INDEXES + N_TEST_SAMPLE_INDEXES,\n",
    "        stratify=y_all,\n",
    "        random_state=SEED + split,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Secondo split: val vs test\n",
    "    val_ids, test_ids, y_val, y_test = train_test_split(\n",
    "        temp_ids,\n",
    "        y_temp,\n",
    "        test_size=N_TEST_SAMPLE_INDEXES,\n",
    "        stratify=y_temp,\n",
    "        random_state=SEED + split,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    df_train = df_dataset_reduced[df_dataset_reduced['sample_index'].isin(train_ids)].copy()\n",
    "    df_test = df_dataset_reduced[df_dataset_reduced['sample_index'].isin(test_ids)].copy()\n",
    "\n",
    "    # Normalise features (fit on THIS split's training data)\n",
    "    mm_scaler = StandardScaler()  #MinMaxScaler()\n",
    "    df_train[features_to_normalize] = mm_scaler.fit_transform(df_train[features_to_normalize])\n",
    "    df_train[features_to_normalize] = mm_scaler.transform(df_train[features_to_normalize])\n",
    "    df_test[features_to_normalize] = mm_scaler.transform(df_test[features_to_normalize])\n",
    "\n",
    "    labels_df_split = labels_df[labels_df['sample_index'].isin(test_ids)].copy()\n",
    "\n",
    "    # --- 5. Build test sequences ---\n",
    "    # Use the best window/stride from final_best_params\n",
    "    X_test_cont, X_test_cat, y_test = build_sequences(\n",
    "        df_test,\n",
    "        labels_df_split,\n",
    "        continuous_cols=CONTINUOUS_COLS_REDUCED,\n",
    "        categorical_cols=CATEGORICAL_COLS_REDUCED,\n",
    "        window=final_best_params['window_size'],\n",
    "        stride=final_best_params['stride']\n",
    "    )\n",
    "\n",
    "    # --- 6. Create the Test DataLoader ---\n",
    "    test_ds = TensorDataset(torch.from_numpy(X_test_cont).float(), torch.from_numpy(X_test_cat).long(),\n",
    "                            torch.from_numpy(y_test).long())\n",
    "    test_loader = make_loader(\n",
    "        test_ds,\n",
    "        batch_size=final_best_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Handle empty test sets from user splits\n",
    "    if len(test_ds) == 0:\n",
    "        print(f\"  WARNING: Test set for split {split + 1} is empty. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- 7. Initialise the Model ---\n",
    "    # Use the best architecture parameters from the grid search\n",
    "    model = RecurrentClassifier(\n",
    "        continuous_input_size=len(CONTINUOUS_COLS_REDUCED),\n",
    "        categorical_cardinalities=categorical_cardinalities,\n",
    "        embedding_dims=embedding_dims,\n",
    "        hidden_size=final_best_params['hidden_size'],\n",
    "        num_layers=final_best_params['hidden_layers'],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=final_best_params['dropout_rate'],\n",
    "        bidirectional=final_best_params['bidirectional'],\n",
    "        rnn_type=final_best_params['rnn_type']\n",
    "    ).to(device)\n",
    "\n",
    "    # --- 8. Load the model weights for this specific split and config ---\n",
    "    # Fix: Use best_config_str directly as it already holds the experiment name or combined name.\n",
    "    model_path = f\"models/{best_config_str}/split_{split}_model.pt\"\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Model file not found at {model_path}\")\n",
    "        print(f\"  Skipping split {split + 1}.\")\n",
    "        continue\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # --- 9. Esecuzione inferenza sul test set (continue + categoriche) ---\n",
    "    split_test_preds, split_test_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_cont, x_cat, y in test_loader:\n",
    "            x_cont = x_cont.to(device)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x_cont, x_cat)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            split_test_preds.append(preds)\n",
    "            split_test_targets.append(y.cpu().numpy())\n",
    "\n",
    "    split_test_preds = np.concatenate(split_test_preds)\n",
    "    split_test_targets = np.concatenate(split_test_targets)\n",
    "\n",
    "    # --- 10. Calculate and store metrics for this split ---\n",
    "    split_test_acc = accuracy_score(split_test_targets, split_test_preds)\n",
    "    split_test_prec = precision_score(split_test_targets, split_test_preds, average='weighted', zero_division=0)\n",
    "    split_test_rec = recall_score(split_test_targets, split_test_preds, average='weighted', zero_division=0)\n",
    "    split_test_f1 = f1_score(split_test_targets, split_test_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"F1 Score on test set for Split {split + 1}: {split_test_f1:.4f}\")\n",
    "\n",
    "    test_accuracies.append(split_test_acc)\n",
    "    test_precisions.append(split_test_prec)\n",
    "    test_recall_scores.append(split_test_rec)\n",
    "    test_f1_scores.append(split_test_f1)\n",
    "\n",
    "    all_test_targets.extend(split_test_targets)\n",
    "    all_test_preds.extend(split_test_preds)\n",
    "\n",
    "# --- 11. After the loop: Print mean metrics and plot confusion matrix ---\n",
    "print(\"\\nAverage metrics across all splits on the test set:\")\n",
    "print(f\"Mean Accuracy: {np.mean(test_accuracies):.4f} \\u00b1 {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(test_precisions):.4f} \\u00b1 {np.std(test_precisions):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(test_recall_scores):.4f} \\u00b1 {np.std(test_recall_scores):.4f}\")\n",
    "print(f\"Mean F1 score: {np.mean(test_f1_scores):.4f} \\u00b1 {np.std(test_f1_scores):.4f}\")\n",
    "\n",
    "# Generate confusion matrix for the concatenated test sets\n",
    "cm = confusion_matrix(all_test_targets, all_test_preds)\n",
    "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Visualise confusion matrix\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=labels, fmt='',\n",
    "            cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Aggregated Confusion Matrix ‚Äî Test Sets Across Splits')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "341580bea37494d1",
   "metadata": {
    "id": "341580bea37494d1"
   },
   "source": [
    "## **Inference on kaggle dataset**"
   ]
  },
  {
   "cell_type": "code",
   "id": "32643d925c26f316",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32643d925c26f316",
    "outputId": "94cbbb00-ffe2-4726-e95d-231a0a8442de"
   },
   "source": [
    "# --- 1. Combine fixed and best hyperparameters ---\n",
    "# 'fixed_params' and 'best_config' are loaded from the grid search cell\n",
    "final_best_params = {**fixed_params, **best_config}\n",
    "\n",
    "# Generate config string (from grid params only) to find saved model files\n",
    "# If best_config is empty (no params tuned), use EXPERIMENT_NAME directly as the key\n",
    "if not best_config:\n",
    "    best_config_str = EXPERIMENT_NAME\n",
    "else:\n",
    "    best_config_str = EXPERIMENT_NAME + \"_\".join([f\"{k}_{v}\" for k, v in best_config.items()]).replace('.', 'p')\n",
    "\n",
    "# 1) Ricava la config migliore e lo split migliore\n",
    "split_scores = [results[best_config_str].get(f\"split_{i}\", -1.0) for i in range(K)]\n",
    "best_split_idx = int(np.argmax(split_scores))\n",
    "\n",
    "print(f\"Miglior configurazione: {best_config_str}\")\n",
    "print(f\"Miglior split: {best_split_idx} con F1={split_scores[best_split_idx]:.4f}\")\n",
    "\n",
    "# 2) Istanzia il modello con gli iperparametri migliori\n",
    "best_model = RecurrentClassifier(\n",
    "    continuous_input_size=len(CONTINUOUS_COLS_REDUCED),\n",
    "    categorical_cardinalities=categorical_cardinalities,\n",
    "    embedding_dims=embedding_dims,\n",
    "    hidden_size=final_best_params['hidden_size'],\n",
    "    num_layers=final_best_params['hidden_layers'],\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=final_best_params['dropout_rate'],\n",
    "    bidirectional=final_best_params['bidirectional'],\n",
    "    rnn_type=final_best_params['rnn_type']\n",
    ").to(device)\n",
    "\n",
    "# 3) Carica i pesi del modello migliore e lo scaler associato\n",
    "best_model_path = f\"models/{best_config_str}/split_{best_split_idx}_model.pt\"\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "best_scaler_path = f\"models/{best_config_str}/split_{best_split_idx}_scaler.pkl\"\n",
    "scaler = joblib.load(best_scaler_path)\n",
    "\n",
    "print(f\"Modello caricato da {best_model_path}\")\n",
    "print(f\"Scaler caricato da {best_scaler_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bb894d6b0be0474",
   "metadata": {
    "id": "9bb894d6b0be0474"
   },
   "source": [
    "submission_path = f\"submissions/{EXPERIMENT_NAME}_submission.csv\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95a5215cfc279e5d",
   "metadata": {
    "id": "95a5215cfc279e5d"
   },
   "source": [
    "# 4) Funzione per creare le sequenze per l'inferenza (senza etichette)\n",
    "def build_sequences_inference(df, continuous_cols, categorical_cols, window=200, stride=200):\n",
    "    assert window % stride == 0\n",
    "    X_cont, X_cat, owners = [], [], []\n",
    "    for sid, g in df.groupby('sample_index'):\n",
    "        cont = g[continuous_cols].values\n",
    "        cat = g[categorical_cols].values\n",
    "        pad = (window - (len(cont) % window)) % window\n",
    "        if pad > 0:\n",
    "            cont = np.concatenate([cont, np.zeros((pad, cont.shape[1]), dtype='float32')], axis=0)\n",
    "            cat = np.concatenate([cat, np.zeros((pad, cat.shape[1]), dtype='int64')], axis=0)\n",
    "        i = 0\n",
    "        while i + window <= len(cont):\n",
    "            X_cont.append(cont[i:i + window])\n",
    "            X_cat.append(cat[i:i + window])\n",
    "            owners.append(sid)\n",
    "            i += stride\n",
    "    return np.asarray(X_cont, dtype=np.float32), np.asarray(X_cat, dtype=np.int64), np.asarray(owners, dtype=np.int32)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef4b4f4460e8a27d",
   "metadata": {
    "id": "ef4b4f4460e8a27d"
   },
   "source": [
    "# 5) Normalizza il kaggle test con lo scaler CORRETTO (caricato sopra), quello relativo allo split migliore\n",
    "kaggle_test_df_reduced[features_to_normalize] = scaler.transform(kaggle_test_df_reduced[features_to_normalize])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 6) Costruisci le sequenze per Kaggle test\n",
    "Xk_cont, Xk_cat, owners = build_sequences_inference(\n",
    "    kaggle_test_df_reduced,\n",
    "    continuous_cols=CONTINUOUS_COLS_REDUCED,\n",
    "    categorical_cols=CATEGORICAL_COLS_REDUCED,\n",
    "    window=final_best_params['window_size'],\n",
    "    stride=final_best_params['stride']\n",
    ")\n",
    "\n",
    "# 7) Inference sui windows\n",
    "kaggle_ds = TensorDataset(\n",
    "    torch.from_numpy(Xk_cont).float(),\n",
    "    torch.from_numpy(Xk_cat).long()\n",
    ")\n",
    "kaggle_loader = make_loader(kaggle_ds, batch_size=final_best_params['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb_cont, xb_cat in kaggle_loader:\n",
    "        xb_cont = xb_cont.to(device)\n",
    "        xb_cat = xb_cat.to(device)\n",
    "        logits = best_model(xb_cont, xb_cat)\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "all_preds = np.concatenate(all_preds) if len(all_preds) else np.array([], dtype=np.int64)\n",
    "\n",
    "# 8) Aggrega per sample_index (maggioranza)\n",
    "preds_per_sample = {}\n",
    "for sid, p in zip(owners, all_preds):\n",
    "    preds_per_sample.setdefault(int(sid), []).append(int(p))\n",
    "\n",
    "final_idx = {sid: Counter(v).most_common(1)[0][0] for sid, v in preds_per_sample.items()}\n",
    "\n",
    "# 9) Mappa a etichette testuali e crea submission\n",
    "inv_label_map = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
    "submission = pd.DataFrame({\n",
    "    'sample_index': list(final_idx.keys()),\n",
    "    'label': [inv_label_map[int(v)] for v in final_idx.values()]\n",
    "}).sort_values('sample_index', kind='stable')\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission salvata in {submission_path}\")\n",
    "\n"
   ],
   "id": "2b2fc0bab8e9b0c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bfeda8327961fe3f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
